\chapter{Conclusion}
\label{Ch:conclusion}

For Q-Learning, more training does not guarantee better performance both for training and testing. In the regression model, training the model by using longer days as a period but testing with shorter days as a period draws a better performance. 

For Deep Q Network, the reason why training results are good while testing results are bad is not clear, but one possible interpretation is that it is because the states, which are price changes for past 30 days, have no relationship with future price change. If this is true, it can explain why training results are good while testing results are bad. From this view, training results are good because when the computer decides the action, it already knows the reward, which means it knows future price as well. However, when the computer applies the model from training to testing, it does not know the future price and it decides action by the model itself only, whose input is only the past price. And if the past price has no relationship with the future price, the model is useless and this can be the reason why training result is good whereas testing result is not good. 

Therefore, our conclusion is that it’s hard to predict future price using the past price history. We tried to train the model with various stocks and test with the other stock pairs, and train the model for one stock pair’s 10 year data and test with another 1 year. However, both of test results were not good. We concluded that although how we choose the dataset affects the performance, it is not crucial, rather the model itself has some problem. 

