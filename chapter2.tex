\chapter{Methodology for Q-learning and Deep Q Network}
\label{Ch:methodology}

The main methodologies we are using for the research are Q-Learning and Deep Q Network(DQN). Here are some explanations about those methodologies.

\section{Q-learning}
Q-Learning is one of the Reinforcement Learning algorithms that attempts to learn the value of being in a given state, and taking a specific action there. Q-table is a table of values for every state(row) and action(column) possible in the environment. Within each cell of the table, we learn a value for how good it is to take a given action within a given state.  We start by initializing the table to be all zeros, and then as we observe the rewards we obtain by taking various actions, we update the table accordingly.

When updating the Q-table, Bellman equation is used. The concept of Bellman equation is that the expected long-term reward for a given action in a given state is equal to the immediate reward gained from the current action plus the expected reward from the best future action taken at the following state. Q-table is used to estimate the long-term reward. As shown below, Q-value for a given state(s) and action(a) should equal to the current reward(r) plus the maximum discounted($\gamma$) future reward expected according to Q-table for the next state$(s’)$ we would end up in. 
$$Q(s,a) = r + \gamma (\max (Q(s’,a’))$$

In Q-learning, for every training process, we first check whether the state already exists in the q-learning table. If it exists, we choose the action that has the largest Q-value. If it does not exist, we build a new empty line of this state and randomly choose action. Then we update the value in the Q-learning table by  Q-table learning formula .

As we use Q-learning model, we need to discretize the states and actions. For actions, we discretize it by choosing actions as the weight of stock A of the total portfolio value and only keep one decimal. So we define the action vector as $(0, 0.1, 0.2, …, 0.9, 1)$. For the states, we tried two models. We first use the pair of stocks’ close price, but it is not general enough to be used as states in Q-learning model. Therefore, we generalize the states by using the pair of stocks' close price change, and only keep two decimals. After that, we generalize our states by using another method. We using the slopes of the linear regression functions of the each fixed period(3 days, 10 days) of stocks.

For the reward function, at the first we simply used portfolio value change as a reward. However, as for portfolio management, we should not only consider the profit, but the risk. Thus we use sharpe ratio as the model’s reward function.

\subsection{Q-learning Algorithm}
\def\skipl{0.2in}
\vspace{\skipl}
\fbox{
\begin{minipage}{5in}
Initialising $Q(s,a)$ arbitrarily 

Repeat (for each episode):

Initialise $s$

Repeat (for each step of a episode):

Choose $a$ from $s$ using policy derived from $Q$ (e.g. $\epsilon$ greedy)

Take action $a$, observe $r$, $s’$
\begin{equation}\label{eq:q-update}
Q(s,a) \leftarrow Q(s,a)+\alpha (r+\gamma \max_{a'} Q(s',a')-Q(s,a))
\end{equation}
\begin{equation}\label{eq:s-update}
 s \leftarrow s'
\end{equation}
Equation \eqref{eq:q-update} and \eqref{eq:s-update} are the update equations.
\end{minipage}}
\vspace{\skipl}

\section{Deep Q Network (DQN)}
Deep Q Network uses the techniques from deep learning to approximates Q-value, since in Q-learning both state and action state need to be discrete and calculating and optimizing Q-value is both time and memory consuming. The key is that we apply the deep neural network to approximate the Q-function. We know that neural network is used to find out the right weights by the back propagation process so it can be used to map all state-action pairs to rewards. One standard example for neural network is using the Convolutional Neural Network (CNN). 

Due to the problem of correlation between states and non-stationary targets, when we train the neural network, we store transition in memory M, and randomly sample mini-batch from M and replay to solve the problem. Plus, we separate the target network and copy the network regularly to solve non-stationary targets problem. 
\def\skipl{0.2in}
\vspace{\skipl}
\fbox{
\begin{minipage}{6in}
Initialise replay memory $D$ to capacity $N$

Initialise action-value function $Q$ with random weights $\theta$

Initialise target action-value function $\hat{Q}$ with weights $\theta^- = \theta$

For episode =1, $M$ do

\ \ Initialise sequence $s_1$ = $\{ x_1\}$ and preprocessed sequence $\phi_1 = \phi (s_1)$

\ \ For $t=1$,$T$ do

\ \ \ \ With probability $\epsilon$ select a random action $a_t$ 

\ \ \ \ otherwise select $a_t = argmax_a Q( \phi (s_t) , a \;\theta)$

\ \ \ \ Execute action $a_t$ in emulator and observe reward $r_t$ and image $x_{t+1}$ 

\ \ \ \ Set $s_{t+1}=s_t,a_t,x_{t+1}$ and preprocess $\phi_{t+1} = \phi (s_{t+1})$

\ \ \ \ Store transition $(\phi_t,a_t,r_t,\phi_{t+1})$ in D

\ \ \ \ Sample random minibath of transitions $(\phi_j,a_j,r_j,\phi_{j+1})$ from $D$

$$
Set \ y_j=
\begin{cases}
r_j,& \text{if episode terminates at step j+1} \\
r_j + \gamma max_{a'} \hat{Q}(\phi_{j+1},a';\theta^-)) & \text{otherwise}
\end{cases}
$$

\ \ \ \ Perform a gradient descent step on $(y_j - Q(\phi_j,a_j;\theta))^2$ with respect to network 

\ \ \ \ parameter $\theta$

\ \ \ \ Every C steps reset $\hat{Q}=Q$

\ \ End For

End for

\end{minipage}}
\vspace{\skipl}